{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "There are things like the ideological significance of toenails, which few people talk about and few should. There are things like half-exponential functions , which few people talk about but many should. There are things like Deflategate or manspreading or the dresses worn at the Oscars, which many people talk about but few should. And then there are things like World War II, global warming, black holes, or machine learning, which many people talk about and probably many should. It’s always hard to judge whether a thing in the fourth category is “overhyped” or not: presumably that would require tot Continue Reading There are things like the ideological significance of toenails, which few people talk about and few should. There are things like half-exponential functions , which few people talk about but many should. There are things like Deflategate or manspreading or the dresses worn at the Oscars, which many people talk about but few should. And then there are things like World War II, global warming, black holes, or machine learning, which many people talk about and probably many should. It’s always hard to judge whether a thing in the fourth category is “overhyped” or not: presumably that would require totting up all the hype about the topic, and also everything that’s genuinely interesting or important about it, to see whether the first outweighs the second (but even if so, how much does that matter?). Right now, machine learning offers deep scientific questions (e.g., why does deep learning work as well as it does?), and also an obvious and increasing impact on civilization (from the impending shift to self-driving cars alone, even if we ignored all the other stuff). I’d be tempted to work in machine learning myself, if I weren’t doing quantum computing. (Indeed, I started out in AI and machine learning, as an undergrad at Cornell with Bart Selman and then as a grad student at Berkeley with Mike Jordan, before shifting into quantum computing, where I felt like my “comparative advantage” was greater.) The progress in ML in the past decade—-the progress that led to stuff like IBM Watson, AlphaGo, etc.—-strikes me as genuine and astounding. But at least according to the ML researchers I know, the recent progress has not involved any major new conceptual breakthroughs: it’s been more about further refinement of algorithms that already existed in the 70s and 80s, and of course, implementing those algorithms on orders-of-magnitude faster computers and training them with orders-of-magnitude more data. On the one hand, the fact that decades-old ideas (e.g., backpropagation and its variants) have shown such power when scaled up is of course cause for optimism that we could soon achieve even more amazing feats of AI, by trying the same techniques with yet faster computers and yet larger datasets! But on the other hand, it’s also a reminder that, if we want to know what’s going to matter decades from now, we might need to look around for the analogues today of backpropagation in the 70s and 80s—-i.e., for the ideas being pursued by a few academic oddballs, which are too new and weird and unproven to have attracted much VC funding or glossy magazine articles, and which many people dismiss for what might be merely contingent reasons of technology rather than anything fundamental. In the end, I suppose it’s less interesting to me to look at the sheer amount of machine learning hype than at its content. Like, almost everyone in the 1950s knew that computers were going to be important, and of course they were right, but they were often wildly wrong about the reasons (e.g., dramatically underestimating the difficulty of humanoid robots, while failing to foresee PCs or the Internet). There’s no doubt in my mind that people 30 years from now will agree with us about the central importance of ML, but which aspects of ML will they rage at us for ignoring, or laugh at us for obsessing about when we shouldn’t have? I don’t know the answers to those questions, but I know that those are the things I’d like to know."}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Communicating fluently in English is a gradual process, one that takes a lot of practice and time to hone. In the meantime, the learning process can feel daunting: You want to get your meaning across correctly and smoothly, but putting your ideas into writing comes with the pressure of their feeling more permanent. This is why consistent, tailored suggestions are most helpful for improving your English writing abilities. Seeing specific writing suggestions based on common grammatical mistakes multilingual speakers make in English is key to improving your communication and English writing fluen Continue Reading Communicating fluently in English is a gradual process, one that takes a lot of practice and time to hone. In the meantime, the learning process can feel daunting: You want to get your meaning across correctly and smoothly, but putting your ideas into writing comes with the pressure of their feeling more permanent. This is why consistent, tailored suggestions are most helpful for improving your English writing abilities. Seeing specific writing suggestions based on common grammatical mistakes multilingual speakers make in English is key to improving your communication and English writing fluency. Regular feedback is powerful because writing in a language that isn’t the first one you learned poses extra challenges. It can feel extra frustrating when your ideas don’t come across as naturally as in your primary language. It’s also tough to put your writing out there when you’re not quite sure if your grammar and wording are correct. For those communicating in English in a professional setting, your ability to write effectively can make all the difference between collaboration and isolation, career progress and stagnation. Grammarly Premium helps multilingual speakers sound their best in English with tailored suggestions to improve grammar and idiomatic phrasing. Especially when you’re writing for work, where time often is in short supply, you want your communication to be effortless. In addition to offering general fluency assistance , Grammarly Premium now includes tailored suggestions for writing issues common among Spanish, Hindi, Mandarin, French, and German speakers, with more languages on the way. Features for all multilingual speakers Grammarly’s writing suggestions will catch the most common grammatical errors that multilingual speakers make in English. For example, if you drop an article or misuse a preposition (such as “on” instead of “in”), our sidebar will flag those mistakes within the Fix spelling and grammar category with the label Common issue for multilingual speakers . Most importantly, it will provide suggestions for fixing them. While these errors seem small, one right after another can make sentences awkward and more difficult to absorb. Eliminating them all in one fell swoop is a powerful way to put a more fluent spin on your document. Features for speakers of specific languages With Grammarly Premium , speakers of French, German, Hindi, Mandarin, and Spanish can get suggestions specifically tailored to their primary language, unlocking a whole other level of preciseness in written English. For speakers of those languages, our sidebar will flag “false friends,” or cognates, which are words or phrases that have a similar form or sound in one’s primary language but don’t have the same meaning in English. But now Grammarly Premium’s writing suggestions will catch these types of errors for you and provide suggestions on how to fix them. You can find these suggestions in the Sound more fluent category in our floating sidebar. Simply click on the suggestion highlighted in green, and voila, your English will be more polished and accurate. PS: Tailored suggestions for other language backgrounds are on the way!"}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Remember Blockchains and Cryptocurrencies ? What about IoT (Internet of Things), Virtual Reality, Quantum Computing ? Surely, you must have heard of Cloud Computing ? Notice, something common among them? All of them had an immense amount of hype behind them. Folks claimed that these would bring revolutions in the way our world works. A couple of years later some of them have fizzled out, a few still remain in progress, however, only a select few have actually left a mark. Artificial Intelligence , especially its sub-field Machine Learning , has been making news for more than a decade now. Numerous folks Continue Reading Remember Blockchains and Cryptocurrencies ? What about IoT (Internet of Things), Virtual Reality, Quantum Computing ? Surely, you must have heard of Cloud Computing ? Notice, something common among them? All of them had an immense amount of hype behind them. Folks claimed that these would bring revolutions in the way our world works. A couple of years later some of them have fizzled out, a few still remain in progress, however, only a select few have actually left a mark. Artificial Intelligence , especially its sub-field Machine Learning , has been making news for more than a decade now. Numerous folks, at varying points in time, have claimed it’s the same hype-cycle being repeated as usual, and it would meet its end soon. However, we still see interest in the field rising over time. To choose a side in this debate, ask yourself whether the present hype is justified by its impact on our lives. To aid this, imagine this small (and a tad bit biased) segment of a regular day in your life, You are woken up in the morning by an alert from Google Assistant, reminding you of a commitment later in the day. Before leaving the bed, you decide to catch up on updates. You pick up your smartphone and murmur, “Okay Google. Open Facebook”. You laugh at the funny filters used by your friends, amused at how realistic the filters and objects look on their faces. You upload the photo and are surprised the app tags all your friends correctly without any help. You continue scrolling through your feed and see an advertisement. “Great! I really wanted this. Glad that it showed up in my feed.” Your enjoyment is suddenly interrupted by yet another Google Assistant reminder. You hurry up and rush to the washroom. You haven’t even left the bed yet, however, there are more than five ML applications hidden above. Most of us follow such a schedule regularly, without even realizing that such applications would be seen as absurd and impossible just over a decade ago. Machine Learning has silently crept into our lives, changing it in enormous ways. There are numerous other uses which I can’t cover at length e.g. Fraud detection, Personalized Ads, Recommender Systems, Language Translation. If we judge by impact in recent times, I’m sure it’s extremely difficult to name another field which has a higher influence than ML. Having said that, I accept not everything is rosy. There are certain limitations to ML systems. We still require a ‘deeper’ understanding of newer approaches such as Deep Learning. Interpretability of such models is an open challenge. The number of individuals currently entering the field may be more than what is required. There may be other fields which could result in bigger breakthroughs for our civilization, we never know for sure. Regardless of which side of the fence you sit on, you cannot ignore the impact and lure of ML. If there had to be one field which should be overhyped, my vote would surely go to Machine Learning."}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Originally Answered: Do you think machine learning is overrated? (The answer is MY subjective opinion) I used to. Not now. Not anymore. I attempted to solve a problem related to image super-resolution through non-ML way [1]. I did come up with a algorithm that performs reasonably well. But the amount of time I spent on the problem was enormous. Between the conception of the idea and submitting the paper, I spent atleast 6 months (more than that if one also wants to include the time taken for understanding the problem and doing literature survey). After my paper got accepted, I wandered off to loo Continue Reading Originally Answered: Do you think machine learning is overrated? (The answer is MY subjective opinion) I used to. Not now. Not anymore. I attempted to solve a problem related to image super-resolution through non-ML way [1]. I did come up with a algorithm that performs reasonably well. But the amount of time I spent on the problem was enormous. Between the conception of the idea and submitting the paper, I spent atleast 6 months (more than that if one also wants to include the time taken for understanding the problem and doing literature survey). After my paper got accepted, I wandered off to look at other problems. However, the nagging feeling that this problem can be solved using ML (more specifically, Deep Learning) never really went off. So I came back to attack the problem again, this time using ML. I constructed and trained a CNN model and arrived at a better solution than my earlier work! All in 6–8 weeks of time! Enough of ranting about myself. Of late, it is quite evident that longstanding problems are solved more effectively using machine learning. With the availability of enormous amount of data (and computing power too), it is much easier to solve problems by making the computer understand the underlying and invisible information hidden inside the data than hand-crafting features which barely touch upon the surface. Machine learning easily pulls out the pattern hidden in the data which would’ve been a daunting task for humans otherwise. Machine learning is going to stay, for better. And it isn’t overrated at all. [1] Here’s a paper that talks about my work: Image-guided depth map upsampling using normalized cuts-based segmentation and smoothness priors"}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Definitely. Companies are very eager to look ‘modern’ by putting AI into everything they do, even if it adds nothing to their customers. Another aspect that is being overlooked is how much energy it all guzzles. Training an AI takes exorbitant amounts of electricity. It is estimated that by 2027 AI will use up as much electricity as the Netherlands, a highly developed economy of 17.5 million people. Of course, we have seen that problem before with all the useless bit mining of the cryptocurrencies nobody needs except the mafia. Question is whether humankind can afford all this, given that we are Continue Reading Definitely. Companies are very eager to look ‘modern’ by putting AI into everything they do, even if it adds nothing to their customers. Another aspect that is being overlooked is how much energy it all guzzles. Training an AI takes exorbitant amounts of electricity. It is estimated that by 2027 AI will use up as much electricity as the Netherlands, a highly developed economy of 17.5 million people. Of course, we have seen that problem before with all the useless bit mining of the cryptocurrencies nobody needs except the mafia. Question is whether humankind can afford all this, given that we are in the midst of a climate crisis. What is the carbon footprint of these new clothes of the digital emperor?"}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Most of the same fundamentals exist, with some caveats i will deal with later in the answer. If you are working abroad you need to: Invest for the long-term. This will compound your gains and reduce risk Invest and not speculate. A long-term, sensible, plan, is always likely to beat trying to get into short-term trends. We have seen that in recent years with many people getting into meme stocks and crypto at inflated prices. As an expat who might relocate in the future, you need a portable solution as you move around the world - Apply now for a portable, expat-focused, investment plan! Reinvest div Continue Reading Most of the same fundamentals exist, with some caveats i will deal with later in the answer. If you are working abroad you need to: Invest for the long-term. This will compound your gains and reduce risk Invest and not speculate. A long-term, sensible, plan, is always likely to beat trying to get into short-term trends. We have seen that in recent years with many people getting into meme stocks and crypto at inflated prices. As an expat who might relocate in the future, you need a portable solution as you move around the world - Apply now for a portable, expat-focused, investment plan! Reinvest dividends. This makes a huge difference long-term Be diversified, especially when you are older. Putting all your eggs in one basket doesn’t make sense. The key differences about living abroad are: You usually can’t get access to your home countries social security system. Back home, investing 10% of whatever you make might be enough for retirement, because you are paying into the retirement system. Many expats panic when they are older, after not putting enough aside for themselves and their family in the previous years. Tax-efficient local investment vehicles, such as ISAs in the UK, aren’t usually available for expats, so offshore investing makes more sense for most non-American expats. For American expats, most forms of offshore investing have became too difficult due to PFIC and other rules Many people are on fixed-term contracts overseas. If you are a teacher back home, you can still have a position for life, or at least it is difficult to fire you. Overseas, you are often paid more, but are on two or three year contracts working in teaching, oil & gas, intergovernmental organizations etc. This makes saving and investing even more important. Many investment providers won’t accept for certain overseas countries. Even those who do, often have restrictions if you move again. So, finding a truly portable, global and expat-friendly provider is key. So, the same fundamentals apply, but the urgency is often bigger. For Americans living overseas, the complexity is just bigger. Pained by financial indecision? Want to start investing? Reach out for assistance today! Ways to connect and see my content Forbes - Adam Fayed | CEO - adamfayed.com | Forbes Councils Gain two free expat ebooks today! Click here to become a smarter expat investor in less than an hour!"}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "My short answer would be Yes and No. Before I go any further, I would like to state I'm no machine learning expert; but an enthusiast having worked on building some of the models for problems in academic environment during my masters with 60% of my coursework dedicated to machine learning and related areas like NLP. A quick search on LinkedIn, Google for \"Machine Learning Jobs\" yields a tonne of results; everyone seeking Machine Learning Engineers, Big Data engineers,____________(Slot for any fancy name you can come up with). In reality, I've hardly met anyone in person who claim they work on Continue Reading My short answer would be Yes and No. Before I go any further, I would like to state I'm no machine learning expert; but an enthusiast having worked on building some of the models for problems in academic environment during my masters with 60% of my coursework dedicated to machine learning and related areas like NLP. A quick search on LinkedIn, Google for \"Machine Learning Jobs\" yields a tonne of results; everyone seeking Machine Learning Engineers, Big Data engineers,____________(Slot for any fancy name you can come up with). In reality, I've hardly met anyone in person who claim they work on machine learning AND actually mean it by building models on a day to day basis. Like Mert Celikok mentions, there are very few people who build models with raw data and not barely call API's. But they are very small in number and often highly qualified,experienced, super interested in statistics and data. It's one field which requires very thorough knowledge in statistics, math and leveraging the available computing and data. Very very few people do it in reality compared to people who \"say\" they work on machine learning. Often when people claim they \"work\" on machine learning,  it just means they work on software which \"enables\" machine learning like the data pipelines, calling certain API's(without knowledge of internals) or using frameworks like Hadoop, Spark etc., Often Hadoop is considered Machine Learning(Check any machine learning job requirement that you see around) by a good portion of people.(I can confidently say 5/10 people think Hadoop is Machine Learning). In reality Hadoop/Map-Reduce is just a distributed architecture, framework, paradigm  which lets someone process voluminous data in cluster. Even if you are writing Map-Reduce programs, its just another software engineering role specialized in writing code in two phases which work together to solve the bigger picture on a huge data. Hadoop is not Machine Learning alone; its just one of the many drivers for a production level application of machine learning. Same goes with Spark, Kafka etc., Machine Learning Scientist definitely leverage from such frameworks. However, not everyone who uses/touches Hadoop should classify themselves as \"working on Machine Learning\". In my opinion/understanding, you call it machine learning if it involves analyzing data, understanding the underlying distribution, making educated guess, experimenting different model, tuning parameters of model and the whole art of playing around with models. With my experience in academic environment with  working with some of the problems like - building a sentiment based rating system for text reviews, classifying email as spam and ham, classifying/clustering news articles of similar news type and a few other projects that I have worked in academia, I can only say the fun part with any such predictive systems is with building models; which in my opinion should be called as Machine Learning. What adds surrounding a model(decorative services?) should definitely not be called machine learning- which unfortunately is in practice rampantly in industry today. Having said that, I only want people to be more educated on what they do/claim before they say they work on machine learning. Unless you are building models, playing with data directly, you shouldn't be calling machine learning engineers or someone who works on machine learning in my opinion. The reality that every single person you meet wants to work/already works on machine learning without even having dived into it is what makes it a overrated term. But the wonderful science behind it is definitely not overrated; rather underrated. tl;dr: Not everyone who calls an API is a Machine Learning Engineer!"}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "I’ve seen machine learning overrated in a few ways, both by people with little experience and, more perniciously, people deeply invested in the field. The most common belief is that machine learning is more general and more powerful than it really is. Machine learning is good at things machine learning is good at and, of course, it’s bad at everything else. If you listen to some people though, you’d believe you could throw a neural net at any problem and get a solid solution. I mostly chalk this down to inexperience and misplaced enthusiasm, but it’s also a result of aggressive hype by people wh Continue Reading I’ve seen machine learning overrated in a few ways, both by people with little experience and, more perniciously, people deeply invested in the field. The most common belief is that machine learning is more general and more powerful than it really is. Machine learning is good at things machine learning is good at and, of course, it’s bad at everything else. If you listen to some people though, you’d believe you could throw a neural net at any problem and get a solid solution. I mostly chalk this down to inexperience and misplaced enthusiasm, but it’s also a result of aggressive hype by people who should know better. Karpathy’s recent viral post Software 2.0 is a great example: he makes some interesting points but leaves you with an impression that deep learning is the future of programming. The article somehow elides how problems outside a few niches (vision, speech, NLP, robotics) aren’t clearly amenable to this approach. It’s not just systems software; even most areas of business logic are still better solved by somebody experienced writing a few hundred lines of code than machine learning. If anything gets to be “software 2.0” it’s garbage collection and high-level languages, and deep learning isn’t even “software 3.0”¹. Neural nets are “just ‘another tool in your machine learning toolbox’” and, more importantly, machine learning is just another tool in your programming toolbox ! This has real consequences. I see people putting massive resources into machine learning-based systems when simpler solutions would be both faster to build and more effective. Take the problem of forecasting demand for items at a store. You could try doing this as a pure machine learning system, but the system would struggle—and ultimately fail —to extract all the structure you need from your data. There are a lot of factors that matter, and some aren’t going to show up in data you can realistically work with. We’d be far better off modeling a bunch of things explicitly (demand elasticity based on pricing and promotions) and relying on human experience for others (changes in consumer fashion). The ideal system for solving a lot of hard problems has to be a hybrid: some machine learning based on data, some explicit modeling and some interactive ways to take advantage of experts. But too many people don’t design problems like this because they see machine learning as a panacea and see building a black box that operates solely on data as a goal . I’m not surprised by this state of events. The open secret of research—academic or industrial—is that only the things that work see the light of day. How many problems have numerous teams tried to solve with machine learning an failed? You don’t hear about many of them except through back channels if you chat with active researchers in the field. (One example: I know a bunch of people have tried and failed to apply deep learning to various problems in program synthesis, but only because I heard it through a research grapevine.) A related problem is that people overstate the impact of machine learning in a product. A lot of consumer products now feature machine learning at their core—think of Quora and Facebook’s feed. Since machine learning is the new hotness and deeply technical, the products’ success must be due to machine learning! Thing is, I bet the impact of machine learning is marginal at best: most of the effect is explained by the social design of the tools . What really matters is that Quora has a feed and lets you follow people and topics. I would not be surprised if a much rougher algorithm (perhaps a heuristic-based rules engine) could produce a feed as good as if not better than Quora’s black magic! I use other products that have a similar design to Quora without any “machine learning” (like Reddit) and, frankly, my Reddit front page does a better job surfacing things I care about than Quora does! (The rest of Quora’s design—the non-machine-learny bits—fit me a lot better than Reddit.) One thing I find illuminating is how many quantitative trading shops have not embraced machine learning whole-heartedly. Some have, of course, but a bunch of others continue making obscene amounts of money with relatively straightforward hand-tuned algorithms. Again: a rules engine filled with expert-written heuristics works eerily well! Some strategies were developed or discovered with machine learning techniques (also known as “statistics”) but others are created more thanks to deep domain expertise. My point is not that machine learning is useless for trading: it clearly has its place. Rather the point is that it very much does not rule the roost, contrary to what you might expect just from hype. The final ways I see machine learning being overrated is going to be painfully familiar to anyone who’s tried implementing machine learning systems in production: machine learning is way more fiddly than it seems. You might think you can just apply some machine learning algorithm you’ve heard about to your problem, but chances are it won’t work nearly as well as the blog post or paper you got it from. A lot of details never make it to papers; they exist solely as institutional knowledge among professionals in the field. You’ll need to spend a lot of time configuring the algorithm for your problem, even if your problem is almost identical to the original you’re working from. You’ll need to tune hyperparameters, find the right architecture, preprocess your data in weird ways, maybe even restate parts of your problem… You can’t just throw your problem at an existing algorithm; you’ll either need extensive experience or a lot of trial and error. Machine learning is a powerful, useful set of techniques and has allowed us to solve problems we couldn’t have handled before. The supply chain optimization system I’m working on today, for example, will benefit from adding some machine learning systems on top of the classical operations research foundation we have now. But, all that said, machine learning is nowhere near as general, powerful or impactful as people seem to believe! footnotes ¹ If I had to make a bet on what future technology will be worth calling “software 3.0”, I’d say it’s interactive development with tools backed by program synthesis. But that might just be wishful thinking and it is a long way out!"}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Oh hell yes. There are so many people with a stake in presenting it as more that it currently is. So many ambitious overachievers out there spinning BS to get money. Look, we do not have the ability to create an AI controlling a robot that can peel an orange. We lack ability to do many things a human does. And yet morons like an ex-exec from Google excite the press with claims that AI will replace humans all over in industry. I say the Emperor’s New Clothes are baloney. The ability of NNs to be trained to classify images is just a small part of real AI. So what if there’s a lot of success there Continue Reading Oh hell yes. There are so many people with a stake in presenting it as more that it currently is. So many ambitious overachievers out there spinning BS to get money. Look, we do not have the ability to create an AI controlling a robot that can peel an orange. We lack ability to do many things a human does. And yet morons like an ex-exec from Google excite the press with claims that AI will replace humans all over in industry. I say the Emperor’s New Clothes are baloney. The ability of NNs to be trained to classify images is just a small part of real AI. So what if there’s a lot of success there identifying a truck in an image? That hardly translates into an AI that can think like a human and solve problems. There are no AIs currently that can reason about why your girlfriend suddenly got sullen and quiet with you, or why Billy down the street beat up a cat. Then there’s Ray Kurzweil. I’d like to get him on stage and beat him over the head with a mop for the terrible hype he generates regarding the ‘Singularity’. His time frame is wrong, his view is only an opinion, not fact nor provable fact. He angers me for conning the public into thinking humans are going to be threatened by AI overlords any time soon. Not going to happen. Making AIs as smart as people is exceedingly hard and it’s not soon in the cards. I say this as an AI researcher who works on these problems and I see how truly difficult they are. We can, and will solve some of the problems but we are a long way from AIs that can match human minds. An AI that plays Go better than humans is buildable because the game is a closed small universe. What humans have to work with is an enormous complicated world, and much harder to deal with. So, yes, AI is way hyped, and the promoters may be leading us to the next AI winter if they keep up the crap and eventually burn out expectations."}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Hey, the author here. I wasn’t aware that the book was hyped. Hearing that it might be overhyped surprises me quite a bit! I can observe that people like it. I receive emails and PM’s on LinkedIn almost daily from people who tell me that during and after reading my book their interest in machine learning has grown a lot and now they want to know much more. My book is by no means a bible of ML. I think what people appreciate about this book is that it’s honest about what it is and it delivers. Its title says “Hey, I’m a hundred-page book on ML!” and it doesn’t disappoint when you read it, being c Continue Reading Hey, the author here. I wasn’t aware that the book was hyped. Hearing that it might be overhyped surprises me quite a bit! I can observe that people like it. I receive emails and PM’s on LinkedIn almost daily from people who tell me that during and after reading my book their interest in machine learning has grown a lot and now they want to know much more. My book is by no means a bible of ML. I think what people appreciate about this book is that it’s honest about what it is and it delivers. Its title says “Hey, I’m a hundred-page book on ML!” and it doesn’t disappoint when you read it, being concise but rich both on detail and coverage. I think that Peter Norvig said it all in his endorsement: “ for the reader who understands that this is the first 100 (or actually 150) pages you will read, not the last, [the book] provides a solid introduction to the field. ” It’s a solid introduction book, nothing to hype or especially overhype."}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Yes. The part is being mistaken for the whole: We’ve got people who aren’t seeing the whole yelling about the part they think is the whole. And they may be feeling around and touching the animal’s leg, for example, and saying the leg is AI, but they aren’t seeing the whole AI elephant. Blind men and an elephant - Wikipedia Here are ten ways our approach to AI falls short in today’s world: Statistical methods (machine learning, neural nets, advanced statistics) aren’t enough by themselves. These do perception and learning well, but not abstraction and reasoning. Knowledge representation (set theory Continue Reading Yes. The part is being mistaken for the whole: We’ve got people who aren’t seeing the whole yelling about the part they think is the whole. And they may be feeling around and touching the animal’s leg, for example, and saying the leg is AI, but they aren’t seeing the whole AI elephant. Blind men and an elephant - Wikipedia Here are ten ways our approach to AI falls short in today’s world: Statistical methods (machine learning, neural nets, advanced statistics) aren’t enough by themselves. These do perception and learning well, but not abstraction and reasoning. Knowledge representation (set theory, predicate logic, description logic and other contextualizing types of logic) is underappreciated and needs to be added to the mix much more frequently than it is, so that a contextual map of interlinked, reusable knowledge domains becomes accessible to the AIs. KR does reasoning well and might help with abstraction when added to the statistical methods in #1.Knowledge graphs that serve as an integration/operation layer on top of data lakes are the modern way KR gets added to the mix. Mathematical models and accompanying logics of various types needed to be added at various tiers to the mix. Technologists will get excited about the models they understand and claim their type of model is at the center of the AI universe and explains the whole universe, when it’s only relevant for some things. Laypeople who don’t know any better sometimes just parrot what they hear, amplifying the noise. Systems-level design is key to the success of AI, but not many are able to bring their AI efforts up to the system level yet. It’s extremely hard to take a holistic approach to design that factors in systems, ethics, and considers and sidesteps unintended consequences. We’re working in batch mode on ad-hoc analytics with single-use data, when we should be working much more often with enriched data flows, data enrichment feedback loops and broadly shared, reusable, contextualized data. We need to build out shared, enriched data infrastructure to serve up enough of the right enriched data at the right time in the right place to the right consumers in the right format. It’s a challenge that’s comparable to what the oil and gas industry did with pipelines, refineries, tanker fleets, railcars, tractor-trailer fleets, but digitized. Most of the development effort is trapped in applications, but in the data-centric world of AI and advanced analytics, applications suites just get in the way of the work that needs to be done. We need to put data first and devote most of the resources we’re squandering on trivial, duplicative imperative code in sharable, dynamically evolving declarative code that can constitute a real model-driven development approach that’s accessible, reusable and scalable at the data layer. In general, we need to remove the outdated abstractions in software that were only useful decades ago, before the web. Those are still in place and only add to the confusion, wasting resources. We keep adding to this stack when we need to collapse. We need to harness the full potential of automation , but need to replace the kludge of methods we’ve developed over decades with a system-level design approach that can focus on building out the automation at the data layer. We need to harness the full power of a diverse workforce who can’t and shouldn’t all be data scientists, architects or engineers. There are knowledge engineers, linguists, modelers, scientists and mathematicians, designers, human factors SMEs, builders, evangelists, mavens, connectors, stewards, troubleshooters, and people who just think in different ways than the rest of us who are just sitting on the sidelines. Recruiters and hiring managers need to see the whole elephant too. Leaders will need clear thinking, vision and our help to be able to execute on a holistic AI workplan. For more specifics, see my data-centric and collapsing the stack slide decks at Alan Morrison’s Presentations on SlideShare , as well as Dave McComb’s excellent books: Software Wasteland: How the Application-Centric Mindset Is Hobbling Our Enterprises (Audible Audio Edition): Dave McComb, Randal Schaffer, Technics Publications: Audible Audiobooks The Data-Centric Revolution: Restoring Sanity to Enterprise Information Systems: Dave McComb: 9781634625401: Amazon.com: Books Dave’s company Semantic Arts hosts an annual Data-centric Architecture Forum you’re welcome to join where we try to tackle some of the bigger challenges listed in 1–10. Thanks for the A2A, Gary Goh . P.S. Societal unintended consequences will be inevitable, and AI alone won’t be able to address these consequences. What’s implied here is the need for a collaborative effort that crosses corporate and country boundaries to anticipate and counter what bad can happen when more and more digital power is placed in the hands of individuals or groups who can form and become active in new ways. This is a broader need than so-called ethical or responsible AI; it’s a need for us to address the consequences of technology, period—climate change is just one those consequences. Most technology has been developed in a relative vacuum, without regard to broader, longer-term consequences or knock-on effects."}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Is there such a thing as “maximum hype”? :-) It’s hard to evaluate where the maximum of this “hype” function is, but certainly all objective indicators are that the field is getting far more than its due share of publicity. Just as a data point, the most recent NIPS (oops, NeurIPS?) conference held in Montreal sold out its registration in around 10 minutes from opening bell. I suppose we can look forward to next year’s conference, where it might sell out in 1 minute! Much of this stampede is due to a large number of companies wanting to attend, and the conference is increasingly more a brawling Continue Reading Is there such a thing as “maximum hype”? :-) It’s hard to evaluate where the maximum of this “hype” function is, but certainly all objective indicators are that the field is getting far more than its due share of publicity. Just as a data point, the most recent NIPS (oops, NeurIPS?) conference held in Montreal sold out its registration in around 10 minutes from opening bell. I suppose we can look forward to next year’s conference, where it might sell out in 1 minute! Much of this stampede is due to a large number of companies wanting to attend, and the conference is increasingly more a brawling trade show of close to 10,000 attendees than the august scientific meeting that it used to be in previous decades. It can be hard to make an objective assessment of where the field is, given the non-stop drum roll of mostly delirious articles from the popular press on how the next AI revolution is just around the corner, based on some software release from the usual players. But, for long-term grizzled veterans like me, who’ve been in the field for longer than most of the young researchers have been alive, we tend not to be taken in so easily. Largely, the publicity is unwarranted as there have been really no major conceptual breakthroughs in the field that I can point to. Progress has largely been due to the availability of much larger amounts of data and compute power, and not due to conceptual breakthroughs. Backpropagation, developed over four decades ago, remains the workhorse of deep learning. Little has changed since the early 1980s when feedforward neural nets were developed. Then as now, we understand very little about how the networks work, except now, we have many more layers of confusion, instead of just two. I can take any human who understands language (e.g., English), sit him or her in a room, and explain a task (any task, e.g. pick up all the books on my bookshelf and organize them by some criterion, e.g. year of publication or topic) in 1 minute. Even such a simple trivial task is beyond what any deep learning network can do today, since every task that such a network must be taught has to be laboriously repeated, ad infinitum, until the billions of weights converge. This somewhat pessimistic assessment may surprise many of you, if you spend your time reading the pages of even quality newspapers like the New York Times or magazines like Technology Review. The reporters who write articles for these publications are largely uneducated journalists who tend to bring an uncritical eye to reports of progress. Actually, any human being can perform their own assessment of where the field is, by doing a very simple comparison. I’ll report on my personal experience within the past month. I went home recently to India, and enjoyed the company of a precocious but otherwise normal four year old, who in the course of a few days, simply dazzled me with his creativity and curiosity. Even though he is just four, he is able to communicate in three languages, growing up in a multi-lingual household and culture. He can use a smartphone, operate a TV or iPad, and turn on several appliances in the household. He can effortlessly recognize animals from plastic cutouts that are barely suggestive of their shape (e.g., a plastic cutout of a parrot was recognized instantly from across a dark room). He comes up with his own creative problems, which no adult human would have ever even thought of (e.g., one morning, he connected the garden hose to an outdoor tap, inserted it into a ground floor window, dragged it into the kitchen, and started to run the water. Every day was a marvel to watch his creativity and curiosity. He was able to flawlessly imitate people, make creative analogies, and understand the causality and common-sense physics of the world. He exhibits a full range of emotions, from joy to sadness, from anger to tears. He invents new words on the fly creatively and without any previous training. It’s safe to say that there is no ML or AI system or robot that can come anywhere close to the creativity of such a child, and I see no way forward to achieve such a breakthrough by extending current approaches. Deep learning requires millions of training examples, and children can neither get such a large training corpus nor seem to need them. The world is largely unlabeled data to a child, and they seem to effortlessly extract the necessary structure to learn language, causality, behavior, social skills, and invent new games for themselves, none of which we in AI have the remotest idea how to implement. Tasks like Imagenet are a ridiculous caricature of how children learn to recognize the world around them — no parent carries around 100,000 labeled examples of dogs and cats, and sits every morning going through this catalog of images till their child scores perfectly on them. The wonder of wonders is that a child learns everything simultaneously: there is simply put no separate “now you have to learn language” and “now you need to learn behavior” and “now you must recognize a cat” training sessions. Data simply comes flooding in, and it’s all filed away, smoothly and effortlessly by the child’s developing brain. Till we stop patting ourselves on the back for successes on artificial tasks like Imagenet or Atari video games, and start to truly acknowledge the vast gulf that separates human ability from machines, progress will continue to be slow. The only real comparison that is worth anything is between a child or human and a machine on similar tasks. When such a comparison was finally reported by Tsividis et al. (AAAI 2017) on Atari video games, the results were highly revealing. Randomly selected humans on Amazon Turk could solve Atari games, like Frostbite, about 1000 times faster than the best deep RL methods, achieving performance superior to machines in as little as 15–30 seconds. That’s the gold standard for comparison. Sadly, there are very few such comparisons made today, and this paper is the real result that should make the front pages of the NY Times or Technology Review, but won’t, because their reporters are forgetting to be skeptical journalists, and too eager to play up the hype."}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "Yeah, it is overhyped, though certainly still valuable. Some of the things I laugh about when it comes to these areas are the people out there who talk about fitting a model with least squares being “machine learning”, even though this basic statistical technique has been around forever. I remember when I was first reading about neural networks back in 2012, my dad told me how he tuned neural networks that modeled risk at some big financial company he worked at in the 80s. The thing I realized is there’s a lot of techniques getting rebranded that have been around for a while and have really onl Continue Reading Yeah, it is overhyped, though certainly still valuable. Some of the things I laugh about when it comes to these areas are the people out there who talk about fitting a model with least squares being “machine learning”, even though this basic statistical technique has been around forever. I remember when I was first reading about neural networks back in 2012, my dad told me how he tuned neural networks that modeled risk at some big financial company he worked at in the 80s. The thing I realized is there’s a lot of techniques getting rebranded that have been around for a while and have really only come back due to better computational resources, more data, and some other research related developments. But at the end of the day, data science/machine learning/AI is not the magic bullet today that a lot of the tech media portrays it to be. Tons of non-technical people, from what I have learned by talking to people in my professional network, think AI and Machine Learning can currently be used to solve impossible problems for companies. This is leading non-AI/ML companies to hire people with the data science and machine learning background to try and turn the data they have into some magic mathematical serum that can be used to wreck their competition. The wishes of many of these companies are infeasible and unrealistic and put insane pressure on the data science/ML teams they build to do the impossible. This is a problem and it all stems from the fact that there’s hype about what data science/ML/AI can do today and it’s inaccurate. Not to mention, there’s a lot of research that still needs to be done to really understand some areas of ML that are hyped, like deep learning. My dad is an executive consultant in tech oriented companies and he tells me he sees so many companies who try to use AI to help rebrand their business since it’s a hot area, but they will minimally dip into AI by just using basic statistical learning techniques or just grab Tensorflow and use a deep learning architecture to try and model some dataset they have internally. It’s such a joke, all a function of the hype, and clearly not nearly as great a use for data science/ML/AI as some of the things larger tech companies are doing with that stuff. So yeah, I think that while data science/ML/AI are useful to learn and use, it is indeed overhyped and likely will be for a little while."}
{"instruction": "Is-machine-learning-currently-overhyped", "context": "", "responses": "When I just started my PhD, I attended a talk by someone who had analyzed some genetic markers for leuchemia using random forests, unweighted assemblies of voters, neural networks, support vector machines and whatnot. The prediction accuracies ranged between 90% and 96%. I asked if he had tried with logistic regression also. He didn’t know how to do that but he gave me his data. Logistic regression turned out to give 100% accuracy. And this was even using cross-validation while the ML models had just been evaluated on the training set. In other words, the prediction problem was completely triv Continue Reading When I just started my PhD, I attended a talk by someone who had analyzed some genetic markers for leuchemia using random forests, unweighted assemblies of voters, neural networks, support vector machines and whatnot. The prediction accuracies ranged between 90% and 96%. I asked if he had tried with logistic regression also. He didn’t know how to do that but he gave me his data. Logistic regression turned out to give 100% accuracy. And this was even using cross-validation while the ML models had just been evaluated on the training set. In other words, the prediction problem was completely trivial. You could have trained a dog to crunch the numbers better than the machine learning did. Standard statistical techniques give deterministic parameter estimates, often interpretable parameters, can be validated using a number of well-understood techniques, will give the same results regardless of which of the standard statistical software packages are used. I am not saying that ML is never better than standard statistical techniques. But whenever it is possible to use standard techniques, it should at least be tried. An ML model that has been demonstrated to be better than other ML models but has not been compared to standard statistical models is not something I will take seriously unless there is a theoretical argument why the standard model would be silly."}
