{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "First, an important background citation: Breiman, L. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16:199–231. Supervised Learning Regression: Panik, M. J. 2009. Regression Modeling: Methods, Theory, and Computation with SAS. Boca Raton, FL: CRC Press. (Disclosure: my favorite regression book.) Decision tree: Breiman, L., Friedman, J., Olshen, R., and Stone, C. 1984. Classification and Regression Trees. Belmont, CA: Wadsworth. Random forest: Breiman, L. 2001. “Random Forests.” Machine Learning 45:5–32. Gradient boosting: Friedman Continue Reading First, an important background citation: Breiman, L. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16:199–231. Supervised Learning Regression: Panik, M. J. 2009. Regression Modeling: Methods, Theory, and Computation with SAS. Boca Raton, FL: CRC Press. (Disclosure: my favorite regression book.) Decision tree: Breiman, L., Friedman, J., Olshen, R., and Stone, C. 1984. Classification and Regression Trees. Belmont, CA: Wadsworth. Random forest: Breiman, L. 2001. “Random Forests.” Machine Learning 45:5–32. Gradient boosting: Friedman, J. H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics 29:1189–1232. Neural network: Rumelhart, D. E., Hinton, G. E., and Williams, R. J. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323:533–536. Support vector machine: Cortes, C. and Vapnik, V. 1995. “Support-Vector Networks.” Machine Learning 20:273–297. Naïve Bayes: Friedman, N., Geiger, D., and Goldszmidt, M. 1997. “Bayesian Network Classifiers.” Machine Learning 29:131–163. Neighbors: Cover, T. and Hart, P. 1967. “Nearest Neighbor Pattern Classification.” IEEE Transactions on Information Theory 13:21–27. Gaussian processes: Seeger, M. 2004. “Gaussian Processes for Machine Learning.” International Journal of Neural Systems 14:69–106. Unsupervised Learning A priori rules: Agrawal, R., Imieliński, T., and Swami, A. 1993. “Mining Association Rules between Sets of Items in Large Databases.” ACM SIGMOD Record 22:207–216. k-means clustering: Hartigan, J. A. and Wong, M. A. 1979. “Algorithm AS 136: A k-Means Clustering Algorithm.” Journal of the Royal Statistical Society, Series C  28:100–108. GloVe Term Embeddings : Jeffrey Pennington, Richard Socher, and Christopher D Manning. \"GloVe: Global Vectors for Word Representation.\" Mean shift clustering: Cheng, Y. 1995. “Mean Shift, Mode Seeking, and Clustering.” IEEE Transactions on Pattern Analysis and Machine Intelligence 17:790–799. Spectral clustering: Von Luxburg, U. 2007. “A Tutorial on Spectral Clustering.” Statistics and Computing 17:395–416. Kernel density estimation: Silverman, B. W. 1986. Density Estimation for Statistics and Data Analysis. Vol. 26. Boca Raton, FL: CRC Press. Non-negative matrix factorization: Lee, D. D. and Seung, H. S. 1999. “Learning the Parts of Objects by Non-negative Matrix Factorization.” Nature 401:788–791. Kernel PCA: Schölkopf, B., Smola, A., and Müller, K.-R. 1997. “Kernel Principal Component Analysis.” In Artificial Neural Networks—ICANN'97, 583–588. Berlin: Springer. Sparse PCA :\tZou, H., Hastie, T., and Tibshirani, R. 2006. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15:265–286. Singular value decomposition: Golub, G. H. and Reinsch, C. 1970. “Singular Value Decomposition and Least Squares Solutions.” Numerische Mathematik 14:403–420. Semi-supervised Learning*: Denoising autoencoders: Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A. 2008. “Extracting and Composing Robust Features with Denoising Autoencoders.” Proceedings of the 25th International Conference on Machine Learning. New York: ACM. Expectation maximization: Nigam, K., McCallum, A.K., Thrun, S. and Mitchell, T.  2000. \"Text Classification from Labeled and Unlabeled Documents using EM.\" Machine Learning 39:103-134. Manifold regularization: Belkin, M., Niyogi, P., and Sindhwani, V. 2006. “Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples.” The Journal of Machine Learning Research 7:2399-2434. Transductive support vector machines: Joachims, T. 1999. “Transductive Inference for Text Classification Using Support Vector Machines.” Proceedings of the 16th International Conference on Machine Learning. New York: ACM. Word2Vec Term Embeddings : Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013). *In semi-supervised learning, supervised prediction and classification algorithms are often combined with clustering. The algorithms noted here provide semi-supervised learning solutions directly. Comments and concerns welcome."}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "Communicating fluently in English is a gradual process, one that takes a lot of practice and time to hone. In the meantime, the learning process can feel daunting: You want to get your meaning across correctly and smoothly, but putting your ideas into writing comes with the pressure of their feeling more permanent. This is why consistent, tailored suggestions are most helpful for improving your English writing abilities. Seeing specific writing suggestions based on common grammatical mistakes multilingual speakers make in English is key to improving your communication and English writing fluen Continue Reading Communicating fluently in English is a gradual process, one that takes a lot of practice and time to hone. In the meantime, the learning process can feel daunting: You want to get your meaning across correctly and smoothly, but putting your ideas into writing comes with the pressure of their feeling more permanent. This is why consistent, tailored suggestions are most helpful for improving your English writing abilities. Seeing specific writing suggestions based on common grammatical mistakes multilingual speakers make in English is key to improving your communication and English writing fluency. Regular feedback is powerful because writing in a language that isn’t the first one you learned poses extra challenges. It can feel extra frustrating when your ideas don’t come across as naturally as in your primary language. It’s also tough to put your writing out there when you’re not quite sure if your grammar and wording are correct. For those communicating in English in a professional setting, your ability to write effectively can make all the difference between collaboration and isolation, career progress and stagnation. Grammarly Premium helps multilingual speakers sound their best in English with tailored suggestions to improve grammar and idiomatic phrasing. Especially when you’re writing for work, where time often is in short supply, you want your communication to be effortless. In addition to offering general fluency assistance , Grammarly Premium now includes tailored suggestions for writing issues common among Spanish, Hindi, Mandarin, French, and German speakers, with more languages on the way. Features for all multilingual speakers Grammarly’s writing suggestions will catch the most common grammatical errors that multilingual speakers make in English. For example, if you drop an article or misuse a preposition (such as “on” instead of “in”), our sidebar will flag those mistakes within the Fix spelling and grammar category with the label Common issue for multilingual speakers . Most importantly, it will provide suggestions for fixing them. While these errors seem small, one right after another can make sentences awkward and more difficult to absorb. Eliminating them all in one fell swoop is a powerful way to put a more fluent spin on your document. Features for speakers of specific languages With Grammarly Premium , speakers of French, German, Hindi, Mandarin, and Spanish can get suggestions specifically tailored to their primary language, unlocking a whole other level of preciseness in written English. For speakers of those languages, our sidebar will flag “false friends,” or cognates, which are words or phrases that have a similar form or sound in one’s primary language but don’t have the same meaning in English. But now Grammarly Premium’s writing suggestions will catch these types of errors for you and provide suggestions on how to fix them. You can find these suggestions in the Sound more fluent category in our floating sidebar. Simply click on the suggestion highlighted in green, and voila, your English will be more polished and accurate. PS: Tailored suggestions for other language backgrounds are on the way!"}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "Unfortunately, I don't think there is a “perfect” machine learning book I can recommend. That said, there are some that pretty good. They all have their pros and cons, but if you read/use them all you can get a pretty good machine learning overview. Here are my favorite ones, roughly in order of preference: Goodfellow, Y. Bengio, and Courville Deep Learning While this book is mostly focused on Deep Learning algorithms and approaches, it is actually a pretty good introduction to Machine Learning. There is a full chapter introducing mathematical and statistical concepts, plus another one that intr Continue Reading Unfortunately, I don't think there is a “perfect” machine learning book I can recommend. That said, there are some that pretty good. They all have their pros and cons, but if you read/use them all you can get a pretty good machine learning overview. Here are my favorite ones, roughly in order of preference: Goodfellow, Y. Bengio, and Courville Deep Learning While this book is mostly focused on Deep Learning algorithms and approaches, it is actually a pretty good introduction to Machine Learning. There is a full chapter introducing mathematical and statistical concepts, plus another one that introduces the basics of machine learning. They might lack some of the depth of other books, but they are very practical and up-to-date. I would say this brings this book pretty high on my list of books I would recommend to people learning machine learning, provided they understand that the final goal is not *only* to learn about Deep Learning. David Barber's Bayesian Reasoning and Machine Learning Kevin Murphy's Machine learning: a Probabilistic Perspective Hastie, Tibshirani, and Friedman's The Elements of Statistical Learning Bishop's Pattern Recognition and Machine Learning Mitchell's Machine Learning There are also many good books that focus on one particular topic. For example, Sutton and Barto's Reinforcement Learning is a classic. But, you need a few of those books in order to build a somewhat comprehensive and balanced understanding of the field. Finally, there are many good practical/hands-on books that are a great way to get started if you are comfortable with coding. I really enjoyed Hands-on Machine Learning with Scikit-learn and Tensor Flow . I haven’t read all of Sebastian Raschka’ s Python Machine Learning , but I have heard great things about it too."}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "A few great ones: The Mathematics of Learning: Dealing with Data Tomaso Poggio and Steve Smale http://www.ams.org/notices/200305/fea-smale.pdf Near Optimal Signal Recovery From Random Projections: Universal Encoding Strategies? Emmanuel Candes and Terence Tao http://statweb.stanford.edu/~candes/papers/OptimalRecovery.pdf Topology and Data Gunnar Carlsson http://comptop.stanford.edu/u/preprints/topologyAndData.pdf Semi-Supervised Learning on Riemannian Manifolds Mikhail Belkin, Partha Niyogi http://people.cs.uchicago.edu/~niyogi/papersps/BNMLJ.pdf Stochastic Methods for L1-regularized Loss Minimi Continue Reading A few great ones: The Mathematics of Learning: Dealing with Data Tomaso Poggio and Steve Smale http://www.ams.org/notices/200305/fea-smale.pdf Near Optimal Signal Recovery From Random Projections: Universal Encoding Strategies? Emmanuel Candes and Terence Tao http://statweb.stanford.edu/~candes/papers/OptimalRecovery.pdf Topology and Data Gunnar Carlsson http://comptop.stanford.edu/u/preprints/topologyAndData.pdf Semi-Supervised Learning on Riemannian Manifolds Mikhail Belkin, Partha Niyogi http://people.cs.uchicago.edu/~niyogi/papersps/BNMLJ.pdf Stochastic Methods for L1-regularized Loss Minimization Shai Shalev-Shwartz, Ambuj Tewari http://jmlr.org/papers/volume12/shalev-shwartz11a/shalev-shwartz11a.pdf and its implementation, in graphlab, described here: Parallel Coordinate Descent for L1-Regularized Loss Joseph K. Bradley, Aapo Kyrola , Danny Bickson, Carlos Guestrin http://www.select.cs.cmu.edu/publications/paperdir/icml2011-bradley-kyrola-bickson-guestrin.pdf and also the liblinear implemenation: A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classication Guo-Xun Yuan, Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf Learning the parts of objects by non-negative matrix factorization Daniel D. Lee & H. Sebastian Seung http://www.nature.com/nature/journal/v401/n6755/abs/401788a0.html and the modern form of NMF, which I prefer to use: Convex and Semi-Nonnegative Matrix Factorizations Chris Ding, Tao Li, Michael I. Jordan http://www.cs.berkeley.edu/~jordan/papers/ding-li-jordan.pdf"}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "Other good papers to add the list which give more insights. 1. Data Mining with Big Data: Xindong Wu, Xingquan Zhu, Gong-Qing Wu, and Wei Ding Data mining with big data One of the best papers to start with is Data Mining with Big Data .Big data has been the latest trend and the paper discusses about how data is fetched from large sources in order to avail various operations which include mining. It also discusses about the Characteristics of Big Data and data mining challenges in Big Data. It also introduces how machine learning algorithms helps in mining complex and dynamic data. 2. An initial st Continue Reading Other good papers to add the list which give more insights. 1. Data Mining with Big Data: Xindong Wu, Xingquan Zhu, Gong-Qing Wu, and Wei Ding Data mining with big data One of the best papers to start with is Data Mining with Big Data .Big data has been the latest trend and the paper discusses about how data is fetched from large sources in order to avail various operations which include mining. It also discusses about the Characteristics of Big Data and data mining challenges in Big Data. It also introduces how machine learning algorithms helps in mining complex and dynamic data. 2. An initial study of predictive machine learning analytics on large volumes of historical data for power system applications The paper deals mainly with the advent of machine learning in correlation with power system applications. Analytics in this particular field would require tools and strategies which inculcate machine learning in order to mine knowledge and perform data prediction. The main problem which usually occurs in this context is that large chunks of data usually results in the processor running out of memory. Industrial analytics is the key term coined over here which explains how data collected is stored in huge data repositories in a variant of data structures. Machine learning algorithms and strategies to execute such environments are discussed and knowledge is extracted out of this data. 3. What is the expectation maximization algorithm? http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf Explanation of EM algorithm with good coin tosses example 4. Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier] 5. DBpedia - A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia 6. A Review of Relational Machine Learning for Knowledge Graphs 7. Incremental Learning with Support Vector Machines http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.6367 8. Neural-network-based fuzzy logic control and decision system 9. An Introduction to MCMC for Machine Learning 10. Selection of relevant features and examples in machine learning"}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "It’s always a fun exercise to synthesize 30+ years of research experience in a field, like ML, into compiling some sort of “Best Hits” list. As in every scientific field, the most influential papers can be catalogued in different ways. For example, one can look at objective criteria, such as the papers with the maximum number of citations in some subfield, like deep learning ( terryum/awesome-deep-learning-papers ). One can also look at papers that were singularly influential in some way. Finally, one can simply choose papers that were helpful in clarifying some important idea, or did an excelle Continue Reading It’s always a fun exercise to synthesize 30+ years of research experience in a field, like ML, into compiling some sort of “Best Hits” list. As in every scientific field, the most influential papers can be catalogued in different ways. For example, one can look at objective criteria, such as the papers with the maximum number of citations in some subfield, like deep learning ( terryum/awesome-deep-learning-papers ). One can also look at papers that were singularly influential in some way. Finally, one can simply choose papers that were helpful in clarifying some important idea, or did an excellent job of providing a tutorial of an important area. Each of the papers I have chosen below has been hugely influential not only in the ML field itself, but has also had major impact outside ML. Each has been cited tens of thousands of times, but beyond the citation count, they have each changed the field in some significant way. I previously answered a related question on the top 20 papers published in AI, so feel free to peruse through my earlier top 20 list, which was somewhat broader, and included papers in neuroscience and ML. Which are the top 20 papers related to AI (both machine learning & symbolic), so that I can cover the basics and choose a niche for my research? OK, on to my newer list of the best ML papers ever published…. To begin, as they say, at the beginning, in terms of its sheer impact, it is hard to top the 1967 paper by Gold on “Language Identification in the Limit”. (see https://ac.els-cdn.com/S0019995867911655/1-s2.0-S0019995867911655-main.pdf?_tid=bc318a60-0ae5-4de6-868d-4fd2e58897c0&acdnat=1548034543_3ad4aa11e2fd780dc76a63e05c25442d ). This paper is as important to machine learning as the famous results by Godel on the incompleteness of logic, or the classic results by Church and Turing on the limitations of computability. Gold analyzed an extremely simple, and yet powerful, model of learning, whereby a teacher communicates with a learner, and assumes nothing in terms of the learner’s capabilities. The major result was that the set of context-free languages is not learnable in Gold’s model (which came to be known as inductive inference). This was an earth-shattering result 50 years ago, and influenced entire fields of inquiry, like linguistics. How is that children as young as 2 or 3 learn an unknown language (entirely unrelated to their ethnicity, as Indian children can learn Japanese just as easily as Japanese children can learn Hindi, if they grew up in Japan or India, respectively)? Gold’s paper is still relevant today, in the data-obsessed world of the 21st century, if only to remind us of the inherent limitations in the power of learning. If you aspire to be a data scientist, if you don't understand Gold’s theorem, it’s like being a physicist and not knowing the conservation of energy. It’s THAT important! 17 years after Gold’s classic paper, in the proverbial “Orwellian” year of 1984, Leslie Valiant published a classic paper that led to his winning the Turing award — computer science’s highest honor — several decades later. Valiant’s paper entitled “A Theory of the Learnable” picked up the thread where Gold left off, and introduced two crucial refinements of Gold’s model. In Valiant’s model, subsequently dubbed “Probably Approximately Correct Learning” (or PAC learning), a learner is only required to converge to an approximation of the desired concept, and is also allowed to fail entirely with some (small) probability. These two refinements made Valiant’s model much more realistic — a robot trained on the concept of “trees” in Florida is unlikely to do well in New England, as it might get confused by fall foliage. Valiant’s model allowed for the set of all possible trees to be sampled by some unknown but unchanging distribution, and required the teacher to be “fair”, so that the same training distribution used to teach the robot must be used in testing as well. Valiant’s model is becoming extremely important in the self-driving autonomous car age of today, as car manufacturers are using elaborate simulations of the real world, e.g., a detailed model of Phoenix, to train their cars using reinforcement learning and deep learning. Such a vehicle trained in a simulation of Phoenix would do rather poorly in the topsy turvy streets of San Francisco. PAC learning was combined with the brilliant work of Vapnik and Chervonenkis decades earlier in Russia on statistical learning, and has become one of the cornerstones of modern ML. Valiant’s paper became an instant classic, published not in some august ML journal, but in the breezy trade pamphlet known as the “Communications of the ACM”. It’s probably the most influential scholarly paper in recent decades to be published in CACM ( https://people.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/ValiantLearnable.pdf ) The earliest experimental work on machine learning was by Arthur Samuel on a program that learned to play the game of checkers by purely self play, and it was an astonishing feat, done in the late 1950s on a vacuum tube IBM 701 computer (see picture below). It’s hard to match Samuel’s audacity, since computers were hardly easy to program, as there was no computer display terminals, no modern programming languages, everything had to be coded in assembly, and all one had was some blinking lights. Even today, Samuel’s achievement is hard to match, and it’s arguable that Samuel’s achievement single handedly created the modern experimental field of machine learning. Samuel also invented the term “machine learning” and contributed towards one of the most influential ideas in the field, a method called temporal-difference learning, now a bedrock of the field of reinforcement learning (and used with much effect by Deep Mind in its Atari video game playing demos). Samuel’s paper from 1959, published six decades ago, remains the most influential experimental paper ever published in ML ( http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.368.2254&rep=rep1&type=pdf ) 4. Valiant’s PAC model influenced the development of many algorithms in machine learning, none was more influential than the work by Robert Schapire on “weak learning” that led to the sensational success of the ensemble method known as “boosting”. The fundamental breakthrough in boosting was in showing that instead of building one complex classifier, one could instead build something really simple, like “stumps” in decision trees, that were individually just better than random, but cumulatively could be “boosted” into an ensemble classifier of arbitrary accuracy. Boosting took the world of statistics by storm, and eminent statisticians like Leo Breiman and Trevor Hastie sung its praises. Breiman called boosting a surefire way of turning “a sow’s ear into a silk purse”. Boosting remains as popular as it was when it was first introduced several decades ago, and can be used with any form of machine learning, and practically any method (neural nets, decision trees, linear regression, SVMs, …). The original paper by Schapire remains a classic on “The Strength of Weak Learnability” ( http://rob.schapire.net/papers/strengthofweak.pdf ). 5. Two singular events occurred when I was a graduate student, studying ML, in the 1980s. One was the publication of Valiant’s paper in CACM in 1984. The other was the appearance of the two volume book, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, by Rumelhart et al. in 1986. This book introduced the modern field of neural networks (now termed deep learning). The breakthrough technique, of course, was back propagation, introduced in a chapter in this book and also published as a paper in Nature ( Learning representations by back-propagating errors ). Backpropagation is completely implausible as an explanation of how the brain learns, since there is no evidence from neuroscience that the brain actually computes error gradients by backpropagation — this would be a formidable undertaking given that we have 100 billion neurons and upwards of 10^{12} connections among neurons. But, despite the implausibility of backpropagation, it has become the workhorse of modern deep learning, and stochastic gradient descent seems destined to be around for a lot longer as there are no obvious replacements for it yet. Recent scholarship has uncovered that the original ideas underlying backpropagation were known far earlier than the 1986 Rumelhart paper. No one has been more vocal about this issue than Jurgen Schmidhuber, one of the inventors of LSTM models, which are widely used in modeling sequential problems in ML. Schmidhuber as an entire page discussing this issue, which is worth going over ( Who Invented Backpropagation? ). It is not uncommon in science for issues of priority to gain controversy (for over 150 years, students of evolution have been battling over whether Darwin or Wallace should get primary credit, or joint credit, for the discovery). The 2 volume PDP book is one of the true landmarks of ML, and created the “connectionism” movement, which is essentially what deep learning arose from. 6. Another landmark publication in 1986 was Ross Quinlan’s paper on “Induction of decision trees” ( http://hunch.net/~coms-4771/quinlan.pdf ). Decision trees made a sharp contrast with neural networks, and they complemented each other extremely well. Even today, despite all the hoopla you hear about deep learning, it may surprise many to know that decision trees (and their ensemble counterpart, random forests) remain even more widely used in the Bay Area tech industry than deep learning. The reasons should not be surprising. Decision tree technology is mature, and the resulting model is easily explainable (unlike deep learning neural nets, which are notoriously hard to understand). Quinlan championed decision trees in ML, whereas Breiman, Friedman, Olshen, and Stone popularized CART (classification and regression trees) in statistics. Both approaches merged later, and led to one of the most enduring approaches in machine learning. Decision trees are widely used in practice, from modeling consumer behavior in web analytics to credit card application processing to gauging stock market strategies and medical diagnosis. Unlike neural nets and deep learning, there is a rich theory underlying decision trees, and the book on Statistical Learning by the Stanford trio Hastie, Tibshirani, and Friedman contains an excellent overview of decision trees and random forests. Wonder of wonders, this book is a free download — put this on your tablet or smartphone NOW, and read it forever ( https://web.stanford.edu/~hastie/Papers/ESLII.pdf ). 7. One way to distinguish statistics from machine learning, at least historically, is dimension. ML folks are fond of problems in very high-dimensional spaces, where it is hard to use traditional statistical methods. In particular, for most of the history of statistics, the number of observations (or training examples) greatly exceeded the number of dimensions (e.g., take the classic work on least-squares fit by Gauss on Tycho Brahe and Kepler’s data of the motion of planets). This so-called “small n, large p” regime causes a fundamental issue of overfitting. How best to handle the fact that the number of parameters greatly exceeds the number of training examples? This controversy continues to plague deep learning models today, and Geoff Hinton himself has commented on why he considers this a non-issue because he personally does not consider overfitting to be a serious problem. In this fascinating talk, cleverly titled “Brains, Sex, and Machine Learning”, Geoff Hinton argues why overfitting is not a problem for the brain). However, most statisticians and ML’ers consider overfitting to be a serious problem. In a landmark paper called “Support Vector Networks”, Vapnik and colleagues introduced a revolutionary idea of building machine learning models using the sophisticated tools of convex optimization, whereby sparsity could be guaranteed by exploiting the dual form of ML models ( Support-vector networks ). This idea of moving to the dual formulation has since become one of the most widely used “tricks” in machine learning, and SVMs popularized the use of convex optimization in machine learning. 8. One of the most intriguing problems in machine learning is understanding the space that data lie in. The most common assumption is that the data lie in Euclidean space, because the most common data format is a matrix of rows, where each row is a vector of values of the features. However, from a variety of areas, evidence has been building that most real-world datasets lie not in Euclidean space, but rather on a curved surface in high dimensions, what mathematicians call a “manifold” (not to be confused with engine parts!). Manifolds are a rich topic of study in math, where they have been explored for over a 100 years in fields like differential geometry. Einstein’s major breakthrough was showing that time and space are inextricably linked into a four-dimensional space-time manifold, a revolutionary concept that is still being tested today 100 years after Einstein’s breakthrough paper on general relativity. C.R. Rao, India’s “living Gold” of statistics, wrote a brilliant paper in the 1950s showing that space of probability distributions lies on a Riemannian manifold, a manifold that has the property that at each point, the space of tangents (or derivatives) defines a vector space with a dot product. The first sign of the 21st century in ML was the appearance of two classic papers on “manifold learning”, which led to techniques like local linear embedding (LLE) and ISOMAP, both published in the same issue of Science in 2000. However, the landmark paper in manifold learning was by Belkin and the (late, and very much lamented) Niyogi. Partha Niyogi, an ML genius at the University of Chicago, tragically died of an illness at the height of his powers, as did Sam Roweis, inventor of LLE. Both their contributions, however, live on, and have endured. In particular, the paper “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation” ( http://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf ) was a true classic, bringing into ML the formal power of differential geometric thinking. The Laplacian has been called “the most beautiful object in all of mathematics and physics” (Nelson, Tensor Analysis), and it has been studied by mathematicians and physicists for centuries. Every major equation in physics, e.g. Maxwell’s equation, features the Laplacian. Belkin and Niyogi showed how the discrete graph Laplacian plays an equally important role in nonlinear dimensionality reduction and representation discovery. In the era of social networks and web data, learning from graphs is hugely important, and the graph Laplacian is the single most important concept in this field. 9. Game theoretic ideas have long been part of AI and fields like economics, but only recently they have begun to enter the mainstream of machine learning, and their popularity in deep learning is largely due to the runaway success of the work of Ian Goodfellow and colleagues in GAN models, or generative adversarial networks ( https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf ). Few recent papers have had such an impact in the field of machine learning and beyond, and GANs have introduced the idea of using game theory as a way to build generative models. There are hundreds of variants of GANs, and one reason for their wildfire popularity is that there are many ways to extend the original concept. At this stage, GANs are too new to be entirely sure of their long-lasting impact, but whatever becomes of this line of research, the original work on GANs will remain a classic in the field. The original GAN model is plagued with instabilities and it is quite tricky to train a GAN. Recent variants, like Wasserstein GANs, claim to be better in some ways, but it is safe to say that there is no one approach that seems to dominate all the others. Part of the problem here is that there is no clear metric to test GAN models, except to look at the pretty pictures they generate. It is astonishing to see the faces of “fake celebrities”” in this recent work by nVidia ( All of these faces are fake celebrities spawned by AI ), and in this era of “fake news”, one worries what the eventual fallout from GAN related work will be (can we ever trust an image or a video again, as telling the “truth”?). These quibbles aside, GANs are a true breakthrough in the field, the one clear step beyond the work in neural nets in the 1980s and earlier. 10. Ach, my last selection. As the saying goes, “when you love what you are doing, it’s not work”. Spending 30+ years in ML has not felt like work, but like absolutely a fun romp through crazy ideas. In the beginning, no one, least of all me, thought this would become a full-time job, but heck, it’s not just a full time job, but one that pays well ( Artificial Intelligence Salaries: Paychecks Heading Skyward ). Gee…what should I choose as my last “top 10” paper of ML of all time. If you search for the most influential papers on the web, most of the links take you to a list of papers on deep learning. This is unfortunate, since ML as a field is sooo much bigger than DL, but that's what happens when you are “in a paradigm”. It gets difficult to “think outside the box”. One of the limitations of all the papers discussed above is that they largely focus on learning statistical correlations or building statistical generative models. What is truly remarkable about human learning, even in young children, is that we learn “how the world works”, that is, we learn to distinguish causes from correlations. How we do this is a bit of a mystery, since correlations are symmetric, so it’s quite challenging to go from correlations (like “thunder” and “lightning” are obviously correlated) to causation (“lightning” causes “thunder”, not the other way around!). One of my predictions for the future of ML is that causal approaches to ML are going to becoming far more popular than they have been to date. Unfortunately, while there are an excellent number of books and papers on causal models (e.g., by Rubin, Pearl and others), there’s not an instant classic paper on causal machine learning, one that will set the ML world on fire. Perhaps that's the reason why causal models haven't become much more popular (yet!). There’s a lot of work in psychology, e.g. by the Berkeley cognitive scientist Alison Gopnik, arguing that causality is very much at the core of how children learn. There’s also much evidence that what matters about perception is the concept introduced by Gibson called “affordances” (i.e., animals and humans perceive the world in terms of what possible actions can be done in each situation, which is intimately tied to our notion of causality and to our desire for “controlling” the world). So, I’m going to end my list by choosing a survey article from cognitive science on the importance of causal models for human learning ( http://eccl.mit.edu/papers/gopnikglymoursobeletal.pdf ). This paper argues that children pay close attention to causal effects in the world. So far, learning machines mostly work by associative methods (e.g., the vast majority of deep learning models learn by correlation). The next breakthrough in ML is likely to be a stellar paper that shows why causal learning is more important than learning by correlation. Such a paper has yet to be written. Perhaps, dear Quora reader, you will take this as a challenge to develop a breakthrough causal learning system, and show the rest of us why causality is crucial to our intelligence. Judea Pearl is fond of quoting the ancient Greek philosopher, Democritus, who said “I would rather discover one true cause than gain the kingdom of Persia” ( Democritus - Wikipedia ). Well, there you have it, my selection of the top 10 most influential papers of all time in machine learning (except for the last one, which is kind of a placeholder for a paper that is yet to be written!). If you have had the patience to read through this whole answer, congratulations! I hope you have learned something of the rich history of the ML field. I began in 1982, innocently pursuing the “crazy” idea of building a learning machine, hugely ignorant of almost every paper above, but gripped by a passion that was inexplicable. It has driven the rest of my career, leading me to explore many fields and work with psychologists, biologists, mathematicians, engineers, physicists, chemists, astronomers, and physicians. Everyone collects and analyzes data, and machine learning can be used anywhere (and, to quote a phrase, “there lies the rub”, because one gets carried away into thinking of ML as a universal tool, the “hammer” you use because every problem becomes a “nail”). But, as long as you remember the wisdom of the above papers, and treat the “hammer” with caution, recognizing that often one wants to use other tools besides hammers! May the “ML Force” be with you!"}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "My answer assumes you are a beginner in Machine Learning and have some understanding of Statistics, Probability and Calculus. As mentioned before, Leo Breiman’s two culture paper is a very good start to understand the views of statistical and computer science research community about Statistical Learning. I would highly recommend reading it. For data preparation and preprocessing Hardley Wickham’s Tidy Data is a must read. Regression Analysis is basic and one of the most important concept of Machine Learning, 7 Types of Regression Techniques you should know! can give you an overview of it. Very im Continue Reading My answer assumes you are a beginner in Machine Learning and have some understanding of Statistics, Probability and Calculus. As mentioned before, Leo Breiman’s two culture paper is a very good start to understand the views of statistical and computer science research community about Statistical Learning. I would highly recommend reading it. For data preparation and preprocessing Hardley Wickham’s Tidy Data is a must read. Regression Analysis is basic and one of the most important concept of Machine Learning, 7 Types of Regression Techniques you should know! can give you an overview of it. Very important concept : No free Lunch Theorem . There are several aspects of Machine Learning, namely: Data Representation: Where you transform data to another space (usually vector space). Representing words in vector space : Efficient Estimation of Word Representations in Vector Space . A document to vector : Distributed Representations of Sentences and Documents . Learning: An overview of gradient descent optimization algorithms Comparison of Modern Stochastic Optimization Algorithms A Step by Step Backpropagation Example Evaluation: Empirical evaluation of a model is very important, to understand the model and infer from it. 7 Important Model Evaluation Error Metrics Everyone should know Evaluating Machine Learning Methods CheatSheet Tidbits on Neural Network: Learning: UNDERSTANDING DEEP NEURAL NETWORKS WITH RECTIFIED LINEAR UNITS Regularization: Dropout: A Simple Way to Prevent Neural Networks from Overfitting Application: ImageNet Classification with Deep Convolutional Neural Network s Hope that helps. _/\\_"}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "You have plenty of time. That's a good thing. So, we'll take this slowly and build your foundation. 1. You have some programming background ? That's good. You can easily pick up Python. Usually, for OOP, the concepts are invariant, only the syntax changes. 2. Once you can get around Python comfortably (as in you know the basic data types available etc.), begin scripting elementary algorithms. This is just for practice. Python is a good language to test your algos because you don't have to worry too much about details. After your algo checks out, you can begin worrying about the finer aspects. 3. Continue Reading You have plenty of time. That's a good thing. So, we'll take this slowly and build your foundation. 1. You have some programming background ? That's good. You can easily pick up Python. Usually, for OOP, the concepts are invariant, only the syntax changes. 2. Once you can get around Python comfortably (as in you know the basic data types available etc.), begin scripting elementary algorithms. This is just for practice. Python is a good language to test your algos because you don't have to worry too much about details. After your algo checks out, you can begin worrying about the finer aspects. 3. I prefer understanding and then using concepts rather than just using. Therefore, I cannot stress the importance of learning Mathematics enough.Specifically, you need to have a thorough grounding in Linear Algebra and Analysis. Since you're in 8th, I suggest you begin with the 11th and then the 12th stuff right away (don't worry, you can do it - it's not particularly challenging). Once done, get Strang's Linear Algebra and its Applications and watch his course videos available on MIT OCW. Do the homework problems and the exams. Implement as many algos as possible from what you learn in the course - from Gaussian Elimination to SVD. Then, begin with Apostol's Calculus - both the volumes. After that go through baby Rudin. Then implement algos from Golub's Matrix Computations. 4. After this you can pick up Sheldon Ross' Probability and then Elements of Statistical Learning by Hastie et al. 5. Go to Coursera : Andrew Ng's ML course. If you find it elementary, you may also watch the full lectures available on You Tube. Implement what you learn. That should take care of the basics. Now you may go through Bishop and Hart and all the list of classics commonly available.  After that you may also want to go through the ICML and NIPS papers. EDIT : Therefore : 1. Get a basic handle on Python - implementing simple algos 2. Go through Strang's Linear Algebra and its Applications. Do the homework and exam questions from the OCW. 3. Implement the algorithms from Strang on Python 4. Finish both the volumes of Apostol and then Baby Rudin. 5. Get Golub's Matrix Computations and implement the algos therein. 6. Study Sheldon Ross' Probability and then Elements of Statistical Learning by Hastie et al. 7. After 1-6 : Andrew Ng's ML course on You Tube. 8.  Then the following : Bishop's Pattern Recognition and Machine Learning David Barber's Bayesian Reasoning and Machine Learning Kevin Murphy's Machine learning: a Probabilistic Perspective 9. ICML and NIPS Papers. If there are any specific things you want to ask about, feel free. I'll try my best. EDIT # Some papers you could study after 1-9 : 1. Universal Approximation using Radial-Basis-Function Networks : J. Park and  I. W. Sandberg 2.  Distilling the Knowledge in a Neural Network : Hinton et al 3. A Survey: Time Travel in Deep Learning Space:nAn Introduction to Deep Learning Models and How Deep Learning Models Evolved from the Initial Ideas Haohan Wang and Bhiksha Raj. 4. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning : Collobert and Weston 5. A Neural Probabilistic Language Model : Bengio et al 6. Small Codes and Large Image Databases for Recognition : Torralba et al 7. An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation : Larochelle et al 8.Solving Geometry Problems: Combining Text and Diagram Interpretation : Seo et al 9. Why Does Unsupervised Pre-training Help Deep Learning? : Erhan et al I'll keep you posted."}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "Do not miss this one. A Few Useful things to know about Machine Learning. Pedro Domingos http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf Provides an empirical idea of how Machine Learning should be done. This paper condenses the experience of years of Machine Learning research."}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "I tried to avoid already cited material. I added some hardware links for the interested. Supervised and Unsupervised: Arthur Dempster's Paper wherein the EM Algorithm is introduced. You can get it from MIT and other sites for free. http://www.jstor.org/discover/10.2307/2984875?uid=3739832&uid=2&uid=4&uid=3739256&sid=21106220015431 Information Retrieval: Nevill-Manning, C.G. ; Witten, I.H. (1997). \"Identifying Hierarchical Structure in Sequences: A linear-time algorithm\". A linear-time algorithm Unsupervised Learning: Coates, A.,  Lee, H. and Ng,A . \"An Analysis of Single Layer Networks in Unsuper Continue Reading I tried to avoid already cited material. I added some hardware links for the interested. Supervised and Unsupervised: Arthur Dempster's Paper wherein the EM Algorithm is introduced. You can get it from MIT and other sites for free. http://www.jstor.org/discover/10.2307/2984875?uid=3739832&uid=2&uid=4&uid=3739256&sid=21106220015431 Information Retrieval: Nevill-Manning, C.G. ; Witten, I.H. (1997). \"Identifying Hierarchical Structure in Sequences: A linear-time algorithm\". A linear-time algorithm Unsupervised Learning: Coates, A.,  Lee, H. and Ng,A . \"An Analysis of Single Layer Networks in Unsupervised Learning.\" AISTATS 14, 2011 http://cs.stanford.edu/people/ang/?portfolio=an-analysis-of-single-layer-networks-in-unsupervised-feature-learning Generative Models: Applied to Topic Models. Blei, D.M.; Lafferty, J.D. (2009) Topic Models. http://www.cs.princeton.edu/~blei/papers/BleiLafferty2009.pdf Cross Validation and Overfitting. http://ai.stanford.edu/~ang/papers/cv-final.pdf It's an older paper by Andy Ng . For those of you who use very wide data and do pre-filtering in genomics look at Wikipedia for links to human bias. Hardware: Just because it's good for us to have a notion of what is likely to affect DM and ML. https://www.parallella.org/ Facebook’s open-sourcing of AI hardware is the start of the deep-learning revolution http://en.wikipedia.org/wiki/POWER8#Specifications Also, important to learn is TensorFlow which is Google's open source, Apache2 licensed AI workbench. Page on tensorflow.org And of course Torch... Torch | Scientific computing for LuaJIT."}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "Wu, Xindong, et al. \"Top 10 algorithms in data mining.\" Knowledge and Information Systems 14.1 (2008): 1-37. This paper shows many important algorithms in data mining. You can study this algorithms one by one."}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "The Google Big Table paper: Chang, Fay; Dean, Jeffrey; Ghemawat, Sanjay; Hsieh, Wilson C; Wallach, Deborah A; Burrows, Michael ‘Mike’; Chandra, Tushar; Fikes, Andrew; Gruber, Robert E (2006), \"Bigtable: A Distributed Storage System for Structured Data\", Google Research. The paper that inspired Map Reduce and relates frameworks."}
{"instruction": "What-are-some-of-the-best-research-papers-or-books-for-Machine-learning", "context": "", "responses": "Although I haven't studied a lot from the books below. But still based on their popularity and my own understanding I found these books to be really good. Kevin Murphy's Machine learning: a Probabilistic Perspective Hastie, Tibshirani, and Friedman's The Elements of Statistical Learning Bishop's Pattern Recognition and Machine Learning"}
